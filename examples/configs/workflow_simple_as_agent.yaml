llm: !SET ${templates.llm_openai_chat}

prompt:
  initializer: prompt
  data:
    template: '{question}'

llm_chain:
  initializer: llm_chain_lcel
  data:
    llm: '{{ llm }}'
    prompt: '{{ prompt }}'
    outputs:
    - 'output'

workflow_simple:
  initializer: conditional_graph
  data:
    inline:
      name: SimpleWorkflow
      input_mapping:
        question: question
      schema:
      - name: question  # schema field name
        field_type: str
        required: true
      - name: not_used  # schema field name
        field_type: str
        default: 'this is a not used field'
      - name: response
        field_type: str
      vertices:
      - name: llm_chain
        agent: '{{ llm_chain }}'  # ref like other chains
          # config: llm_chain.yaml    # ref by config file
          # name: llm_chain
        output_mapping:
          response: output

# python cli.py -c workflow_simple_as_agent.yaml -i graph_simple -q "what's you name?" -I question