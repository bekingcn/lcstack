llm: !SET ${templates.llm_openai_chat}

prompt:
  initializer: prompt
  data:
    template: '{question}'

llm_chain:
  initializer: llm_chain_lcel
  data:
    llm: '{{ llm }}'
    prompt: '{{ prompt }}'
    outputs:
    - 'output'

graph_simple:
  initializer: conditional_graph
  data:
    inline:
      name: SimpleGraph
      input_mapping:
        question: question
      schema:
      - name: question  # state field name
        field_type: str
        required: true
      - name: not_used  # state field name
        field_type: str
        default: 'this is a not used field'
      - name: response
        field_type: str
      vertices:
      - name: llm_chain
        agent: '{{ llm_chain }}'  # ref like other chains
          # config: llm_chain.yaml    # ref by config file
          # name: llm_chain
        output_mapping:
          response: output
    # output_parser_args:
      # type: "struct"
      # output_mapping:
      #   response: "response"
      #   question: "question"
    # outputs:
    # - response
    # - question

graph_simple_from_file:
  initializer: conditional_graph
  data:
    workflow_type: conditional
    file: simple_workflow.yaml
# python cli.py -c graph_simple_as_agent.yaml -i graph_simple -q "what's you name?" -I question